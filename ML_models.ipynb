{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e0ee183-e9c0-489c-8796-61b76929bfa2",
   "metadata": {},
   "source": [
    "# Project 3\n",
    "Xingwang Yu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62188cde-b152-4520-87aa-cbc7921a7d62",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dd3289-2fc5-4631-b754-05abb7582553",
   "metadata": {},
   "source": [
    "`Supervised learning`, also known as supervised machine learning, is a subcategory of machine learning. It is defined by its use of labeled datasets to train algorithms that to classify data or predict outcomes accurately.Basically supervised learning is when we teach or train the machine using data that is well labelled, which means some data is already tagged with the correct answer. After that, the machine is provided with a new set of data so that the supervised learning algorithm analyses the training data and produces a correct outcome from labelled data.\n",
    "\n",
    "`Supervised learning` is classified into two categories of algorithms: \n",
    "<br>`Classification`: A classification problem is when the output variable is a category, such as “Red” or “blue” , “disease” or “no disease”.\n",
    "<br>`Regression`: A regression problem is when the output variable is a real value, such as “dollars” or “weight”.\n",
    "\n",
    "\n",
    "\n",
    "`Data Set Information`:\n",
    "\n",
    "To fit supervised learning models, I found a dataset from kaggle https://www.kaggle.com/datasets/aungpyaeap/fish-market?resource=download. \n",
    "This dataset is a record of 7 common different fish species in fish market sales. The dataset includes measurement of weight, lenght of different part (length1, length2, Length3), height and width. \n",
    "\n",
    "of  With this dataset, a predictive model can be performed using machine friendly data and estimate the weight of fish can be predicted.\n",
    "In this datasets, the scientists mearsured the seed grain structure belonging to three different varieties of wheat: Kama, Rosa and Canadian, 70 elements each. To construct the data, seven geometric parameters of wheat kernels were measured from column 1-7, while the column 8 shows the variety type. \n",
    "\n",
    "`Objective`:\n",
    "\n",
    "Try to build different class of supervised learning regession models to prodict the weight of fish.\n",
    "\n",
    "Let's import the data first and convert it to spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5630b85a-700b-40c7-82ba-ca23074becf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fea95837-d772-4900-967c-28a1587eea41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Species</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Length1</th>\n",
       "      <th>Length2</th>\n",
       "      <th>Length3</th>\n",
       "      <th>Height</th>\n",
       "      <th>Width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bream</td>\n",
       "      <td>242.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>25.4</td>\n",
       "      <td>30.0</td>\n",
       "      <td>11.5200</td>\n",
       "      <td>4.0200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bream</td>\n",
       "      <td>290.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>26.3</td>\n",
       "      <td>31.2</td>\n",
       "      <td>12.4800</td>\n",
       "      <td>4.3056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bream</td>\n",
       "      <td>340.0</td>\n",
       "      <td>23.9</td>\n",
       "      <td>26.5</td>\n",
       "      <td>31.1</td>\n",
       "      <td>12.3778</td>\n",
       "      <td>4.6961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bream</td>\n",
       "      <td>363.0</td>\n",
       "      <td>26.3</td>\n",
       "      <td>29.0</td>\n",
       "      <td>33.5</td>\n",
       "      <td>12.7300</td>\n",
       "      <td>4.4555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bream</td>\n",
       "      <td>430.0</td>\n",
       "      <td>26.5</td>\n",
       "      <td>29.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>12.4440</td>\n",
       "      <td>5.1340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Species  Weight  Length1  Length2  Length3   Height   Width\n",
       "0   Bream   242.0     23.2     25.4     30.0  11.5200  4.0200\n",
       "1   Bream   290.0     24.0     26.3     31.2  12.4800  4.3056\n",
       "2   Bream   340.0     23.9     26.5     31.1  12.3778  4.6961\n",
       "3   Bream   363.0     26.3     29.0     33.5  12.7300  4.4555\n",
       "4   Bream   430.0     26.5     29.0     34.0  12.4440  5.1340"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the data \n",
    "dat = pd.read_csv(\"Fish.csv\", header = 0)\n",
    "\n",
    "dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b9fbe400-4a44-4eb0-b24a-93e9d0a1caaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+-------+-------+-------+------+\n",
      "|Species|Weight|Length1|Length2|Length3| Height| Width|\n",
      "+-------+------+-------+-------+-------+-------+------+\n",
      "|  Bream| 242.0|   23.2|   25.4|   30.0|  11.52|  4.02|\n",
      "|  Bream| 290.0|   24.0|   26.3|   31.2|  12.48|4.3056|\n",
      "|  Bream| 340.0|   23.9|   26.5|   31.1|12.3778|4.6961|\n",
      "|  Bream| 363.0|   26.3|   29.0|   33.5|  12.73|4.4555|\n",
      "|  Bream| 430.0|   26.5|   29.0|   34.0| 12.444| 5.134|\n",
      "+-------+------+-------+-------+-------+-------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#convert to spark df\n",
    "fish = spark.createDataFrame(dat)\n",
    "fish.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b82d29-336e-4402-924f-88cb158e32e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Splitting the Data, Metrics, and Models\n",
    "## Evaluation metrics\n",
    "Evaluation metrics explain the performance of the model. An important aspect of evaluation metrics is their capability to discriminate among model results. There are several metrics we could used to evaluation the model performance. I choosed two commonly used metrics`RMSE` and `MAE` to evaluating the performance of regression models.\n",
    "\n",
    "`RMSE`, Root Mean Squared Error, is the square root of the mean squared error between the predicted and actual values. RMSE is the aggregated mean and subsequent square root of these errors, which helps us understand the model performance over the whole dataset. A benefit of using RMSE is that the metric it produces is in terms of the unit being predicted. For example, using RMSE in a house price prediction model would give the error in terms of house price, which can help end users easily understand model performance.\n",
    "\n",
    "`MAE`, Mean Absolute Error, is the average absolute error between actual and predicted values. MAE is the aggregated mean of these errors, which helps us understand the model performance over the whole dataset. MAE is a popular metric to use as the error value is easily interpreted. This is because the value is on the same scale as the target you are predicting for.\n",
    "\n",
    "`Similarities` between `MAE` and `RMSE`:\n",
    "<br>Aside from the fact that they both are error metrics for regression models, the other similarities are:\n",
    "1. Error is given in terms of the value you are predicting for\n",
    "2. The lower the value the more accurate the model is\n",
    "3. The resulting values can be between 0 and infinity\n",
    "\n",
    "`Difference` between `RMSE` and `MAE`:\n",
    "<br>Whilst they both have the same goal of measuring regression model error, there are some key differences that you should be aware of:\n",
    "1. RMSE is more sensitive to outliers\n",
    "2. RMSE penalises large errors more than MAE due to the fact that errors are squared initially\n",
    "3. MAE returns values that are more interpretable as it is simply the average of absolute error\n",
    "\n",
    "## Splitting data\n",
    "When it comes to data analysis, you can split your data into training and testing sets. \n",
    "\n",
    "A train test split is when you split your data into a training set and a testing set. The training set is used for training the model, and the testing set is used to test your model. This allows you to train your models on the training set, and then test their accuracy on the unseen testing set. There are a few different ways to do a train test split, but the most common is to simply split your data into two sets. For example 80% for training and 20% for testing. This ensures that both sets are representative of the entire dataset, and gives you a good way to measure the accuracy of your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ce2c68bc-1d46-4594-8cb1-3812cb6ae545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129 30\n"
     ]
    }
   ],
   "source": [
    "#split dataset into 80/20 taining and testing sets\n",
    "train, test = fish.randomSplit([0.8,0.2], seed = 1)\n",
    "print(train.count(), test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979c6567-f458-4ed7-a14e-a667e95af627",
   "metadata": {},
   "source": [
    "## Models\n",
    "The following regression model will be used to predict fish weight.\n",
    "1. `Linear regression`\n",
    "<br>Linear Regression is an ML algorithm used for supervised learning. Linear regression performs the task to predict a dependent variable(target) based on the given independent variable(s). So, this regression technique finds out a linear relationship between a dependent variable and the other given independent variables. Hence, the name of this algorithm is Linear Regression.\n",
    "2. `LASSO Regression`\n",
    "<br>Lasso regression is a type of linear regression that uses shrinkage. Shrinkage is where data values are shrunk towards a central point, like the mean. The lasso procedure encourages simple, sparse models (i.e. models with fewer parameters). This particular type of regression is well-suited for models showing high levels of muticollinearity or when you want to automate certain parts of model selection, like variable selection/parameter elimination.\n",
    "3. `ElasticNet Regression`\n",
    "<br> Elastic net is a combination of the two most popular regularized variants of linear regression: ridge and lasso. Ridge utilizes an L2 penalty and lasso uses an L1 penalty.\n",
    "4. `Decision Tree`\n",
    "<br>Decision tree builds regression or classification models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes.\n",
    "5. `Gradient-Boosted Trees (GBTs)`\n",
    "<br>In gradient boosting decision trees, we combine many weak learners to come up with one strong learner. The weak learners here are the individual decision trees. All the trees are conncted in series and each tree tries to minimise the error of the previous tree. Due to this sequential connection, boosting algorithms are usually slow to learn, but also highly accurate. In statistical learning, models that learn slowly perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae99876-6207-4746-9dfb-33a154ff6912",
   "metadata": {},
   "source": [
    "# 3. Model fitting\n",
    "Now, let's make tranformatios on data, which could be used in model fitting. I will make two type of transformation: one inlcude interaction terms which will be used in linear models, while another one do not have interaction terms, which could be used in tree based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "190a0a91-6710-4ead-bea1-265eb79c3f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+------------------+-----------------+------------------+-----+--------------------+\n",
      "|Length1|Length2|Length3| Height|         log_width|       interacted|       poly_height|label|            features|\n",
      "+-------+-------+-------+-------+------------------+-----------------+------------------+-----+--------------------+\n",
      "|   23.2|   25.4|   30.0|  11.52|1.3912819026309295|           589.28|          132.7104|242.0|[23.2,25.4,30.0,1...|\n",
      "|   24.0|   26.3|   31.2|  12.48|1.4599165009905044|            631.2|          155.7504|290.0|[24.0,26.3,31.2,1...|\n",
      "|   23.9|   26.5|   31.1|12.3778|1.5467323770179757|633.3499999999999|153.20993284000002|340.0|[23.9,26.5,31.1,1...|\n",
      "|   26.3|   29.0|   33.5|  12.73|1.4941392880706375|            762.7|162.05290000000002|363.0|[26.3,29.0,33.5,1...|\n",
      "|   26.5|   29.0|   34.0| 12.444|1.6358850824489488|            768.5|154.85313600000003|430.0|[26.5,29.0,34.0,1...|\n",
      "+-------+-------+-------+-------+------------------+-----------------+------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import SQLTransformer, VectorAssembler\n",
    "\n",
    "sqlTrans_1 = SQLTransformer(\n",
    "    statement = \"\"\"\n",
    "                SELECT Length1, Length2, Length3, Height, log(Width) as log_width,\n",
    "                Weight as label FROM __THIS__\n",
    "                \"\"\"\n",
    ")\n",
    "\n",
    "sqlTrans_2 = SQLTransformer(\n",
    "    statement = \"\"\"\n",
    "                SELECT Length1, Length2, Length3, Height, log(Width) as log_width,\n",
    "                (Length1 * Length2) as interacted,\n",
    "                (Height * Height) as poly_height,\n",
    "                Weight as label FROM __THIS__\n",
    "                \"\"\"\n",
    ")\n",
    "df = sqlTrans_2.transform(fish)\n",
    "\n",
    "\n",
    "assembler_1 = VectorAssembler(inputCols = [\"Length1\", \"Length2\", \"Length3\", \"Height\", \"log_width\"], outputCol = \"features\", handleInvalid = 'keep')\n",
    "assembler_2 = VectorAssembler(inputCols = [\"Length1\", \"Length2\", \"Length3\", \"Height\", \"log_width\", \"interacted\", \"poly_height\"], outputCol = \"features\", handleInvalid = 'keep')\n",
    "\n",
    "assembler_2.transform(df).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5684ace9-a9a9-4935-9ffd-d62993bff487",
   "metadata": {},
   "source": [
    "## 3.1. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f1c197fd-92d2-4148-9bf8-1da8835765a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[70.08353755479689, 51.23363828168392, dict_values([0.0])],\n",
       " [70.02820192663172, 51.2905742348669, dict_values([0.01])],\n",
       " [70.17034520438938, 52.014761408171424, dict_values([0.05])],\n",
       " [70.42731139753221, 52.46821092605671, dict_values([0.1])],\n",
       " [74.25880502828645, 53.361536846519094, dict_values([1.0])]]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "#create initial LinearRegression model\n",
    "lr = LinearRegression()\n",
    "#create ParaGrid for cross validation\n",
    "paramGrid_lr = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0, 0.01, 0.05,0.1, 1]) \\\n",
    "    .build()\n",
    "pipeline_lr = Pipeline(stages = [sqlTrans_2, assembler_2, lr])\n",
    "\n",
    "#create cross validator for two different metrics\n",
    "lr_cv_1 = CrossValidator(estimator = pipeline_lr,\n",
    "                          estimatorParamMaps = paramGrid_lr,\n",
    "                          evaluator = RegressionEvaluator(metricName='rmse'),\n",
    "                          numFolds=5)\n",
    "lr_cv_2 = CrossValidator(estimator = pipeline_lr,\n",
    "                          estimatorParamMaps = paramGrid_lr,\n",
    "                          evaluator = RegressionEvaluator(metricName='mae'),\n",
    "                          numFolds=5)\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "lr_cv_model1 = lr_cv_1.fit(train)\n",
    "lr_cv_model2 = lr_cv_2.fit(train)\n",
    "\n",
    "#print cv results for model selection\n",
    "lr_list = []\n",
    "for i in range(len(paramGrid_lr)):\n",
    "    lr_list.append([lr_cv_model1.avgMetrics[i], lr_cv_model2.avgMetrics[i], paramGrid_lr[i].values()])\n",
    "lr_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcde4f5-3ed7-41c6-a435-1c7fa58f2400",
   "metadata": {},
   "source": [
    "Metric RMSE and MAE showed same results in choose best parameters. So either models could be considered as best model for linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c27e2a36-6b81-428e-b765-ecd37542f8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_best = lr_cv_model1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563d424b-802c-48d9-92d0-0bf060bac746",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.2. LASSO Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f3a7a436-f3c5-48c4-af41-aafbb27a8072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[70.08353755479689, 51.23363828168392, dict_values([0.0, 1.0])],\n",
       " [72.98550353440467, 52.554758583731044, dict_values([0.01, 1.0])],\n",
       " [73.63979991378065, 52.39935214770154, dict_values([0.05, 1.0])],\n",
       " [73.28490862017793, 52.55172277962646, dict_values([0.1, 1.0])],\n",
       " [76.17374891266998, 54.173500298453746, dict_values([1.0, 1.0])]]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create initial LASSO model\n",
    "la = LinearRegression()\n",
    "#create ParaGrid for cross validation\n",
    "paramGrid_la = ParamGridBuilder() \\\n",
    "    .addGrid(la.regParam, [0, 0.01, 0.05,0.1, 1]) \\\n",
    "    .addGrid(la.elasticNetParam, [1]) \\\n",
    "    .build()\n",
    "pipeline_la = Pipeline(stages = [sqlTrans_2, assembler_2, la])\n",
    "\n",
    "#create cross validator for two different metrics\n",
    "la_cv_1 = CrossValidator(estimator = pipeline_la,\n",
    "                          estimatorParamMaps = paramGrid_la,\n",
    "                          evaluator = RegressionEvaluator(metricName='rmse'),\n",
    "                          numFolds=5)\n",
    "la_cv_2 = CrossValidator(estimator = pipeline_la,\n",
    "                          estimatorParamMaps = paramGrid_la,\n",
    "                          evaluator = RegressionEvaluator(metricName='mae'),\n",
    "                          numFolds=5)\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "la_cv_model1 = la_cv_1.fit(train)\n",
    "la_cv_model2 = la_cv_2.fit(train)\n",
    "\n",
    "#print cv results for model selection\n",
    "la_list = []\n",
    "for i in range(len(paramGrid_la)):\n",
    "    la_list.append([la_cv_model1.avgMetrics[i], la_cv_model2.avgMetrics[i], paramGrid_la[i].values()])\n",
    "la_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcd9536-b576-484a-bbbd-02abff51a8f7",
   "metadata": {},
   "source": [
    "RMSE and MAE showing same results acrossing parameters. Either model will be considered as best model in LASSO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f96eec7e-b2a8-4b61-bbd7-7983fb0b85e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "la_best = la_cv_model1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3a3fb4-91bc-46ee-bead-3a571d50a3b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.3. ElasticNet Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7e499110-4709-4dfb-8192-00e55fea5172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[70.08353755479689, 51.23363828168392, dict_values([0.0, 0.0])],\n",
       " [70.08353755479689, 51.23363828168392, dict_values([0.0, 0.5])],\n",
       " [70.08353755479689, 51.23363828168392, dict_values([0.0, 0.8])],\n",
       " [70.08353755479689, 51.23363828168392, dict_values([0.0, 0.9])],\n",
       " [70.08353755479689, 51.23363828168392, dict_values([0.0, 1.0])],\n",
       " [70.02820192663172, 51.2905742348669, dict_values([0.01, 0.0])],\n",
       " [73.44043576980751, 52.28176419490781, dict_values([0.01, 0.5])],\n",
       " [73.23251364312262, 52.58175372187534, dict_values([0.01, 0.8])],\n",
       " [73.2339282113476, 52.58282421892039, dict_values([0.01, 0.9])],\n",
       " [72.98550353440467, 52.554758583731044, dict_values([0.01, 1.0])],\n",
       " [70.17034520438938, 52.014761408171424, dict_values([0.05, 0.0])],\n",
       " [72.43687679328559, 52.432669906336955, dict_values([0.05, 0.5])],\n",
       " [71.09293708483156, 52.396290552994984, dict_values([0.05, 0.8])],\n",
       " [72.84846501574646, 52.62634198661726, dict_values([0.05, 0.9])],\n",
       " [73.63979991378065, 52.39935214770154, dict_values([0.05, 1.0])],\n",
       " [70.42731139753221, 52.46821092605671, dict_values([0.1, 0.0])],\n",
       " [71.95315454001988, 52.62511982380654, dict_values([0.1, 0.5])],\n",
       " [72.74717392995109, 52.36512790970979, dict_values([0.1, 0.8])],\n",
       " [74.08104303993107, 52.59885856174857, dict_values([0.1, 0.9])],\n",
       " [73.28490862017793, 52.55172277962646, dict_values([0.1, 1.0])],\n",
       " [74.25880502828645, 53.361536846519094, dict_values([1.0, 0.0])],\n",
       " [76.18506333006505, 53.95766877361219, dict_values([1.0, 0.5])],\n",
       " [75.21771009652188, 53.514731749744286, dict_values([1.0, 0.8])],\n",
       " [75.17421258113413, 53.50670369776053, dict_values([1.0, 0.9])],\n",
       " [76.17374891266998, 54.173500298453746, dict_values([1.0, 1.0])]]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create initial ElasticNet model\n",
    "en = LinearRegression()\n",
    "#create ParaGrid for cross validation\n",
    "paramGrid_en = ParamGridBuilder() \\\n",
    "    .addGrid(en.regParam, [0, 0.01, 0.05,0.1, 1]) \\\n",
    "    .addGrid(en.elasticNetParam, [0, 0.5, 0.8, 0.9, 1]) \\\n",
    "    .build()\n",
    "pipeline_en = Pipeline(stages = [sqlTrans_2, assembler_2, en])\n",
    "\n",
    "#create cross validator for two different metrics\n",
    "en_cv_1 = CrossValidator(estimator = pipeline_en,\n",
    "                          estimatorParamMaps = paramGrid_en,\n",
    "                          evaluator = RegressionEvaluator(metricName='rmse'),\n",
    "                          numFolds=5)\n",
    "en_cv_2 = CrossValidator(estimator = pipeline_en,\n",
    "                          estimatorParamMaps = paramGrid_en,\n",
    "                          evaluator = RegressionEvaluator(metricName='mae'),\n",
    "                          numFolds=5)\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "en_cv_model1 = en_cv_1.fit(train)\n",
    "en_cv_model2 = en_cv_2.fit(train)\n",
    "\n",
    "#print cv results for model selection\n",
    "en_list = []\n",
    "for i in range(len(paramGrid_en)):\n",
    "    en_list.append([en_cv_model1.avgMetrics[i], en_cv_model2.avgMetrics[i], paramGrid_en[i].values()])\n",
    "en_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675216d1-4109-47ad-a7d5-43adcf97eee3",
   "metadata": {},
   "source": [
    "In this model fitting, RMSE and MAE choosed different best parameters. As above mentioned, RMSE is more sensitive to outliers. I choose RMSE as major metric, and select model1 as best ElasticNet Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ae22a5ab-bc64-4f44-8355-ff6384c0b9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_best = en_cv_model1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cf744c-da37-453f-944c-19f02c2bfbb3",
   "metadata": {},
   "source": [
    "## 3.4. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a84d026b-5db8-4dec-bae9-f9327ac0aacd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[205.5271650344436, 153.80191051980339, dict_values([1, 4])],\n",
       " [201.95860860517513, 145.38820283023335, dict_values([1, 5])],\n",
       " [197.4496302810485, 152.47646818934797, dict_values([1, 8])],\n",
       " [201.18204097353976, 150.13400760153553, dict_values([1, 10])],\n",
       " [133.03507721541718, 96.555292328788, dict_values([2, 4])],\n",
       " [155.60801536656044, 105.41755708445444, dict_values([2, 5])],\n",
       " [142.7960942020221, 105.08760797053988, dict_values([2, 8])],\n",
       " [138.72181102738412, 96.85440822497256, dict_values([2, 10])],\n",
       " [106.02549090920866, 70.55277440789473, dict_values([5, 4])],\n",
       " [111.3864869289024, 72.68564840097247, dict_values([5, 5])],\n",
       " [102.43959080948294, 63.70976667317384, dict_values([5, 8])],\n",
       " [95.55481511234447, 59.05176390005563, dict_values([5, 10])],\n",
       " [102.58416821775408, 67.58016496021523, dict_values([10, 4])],\n",
       " [109.66238057416153, 70.99627729900513, dict_values([10, 5])],\n",
       " [102.86740776100396, 60.545728362200144, dict_values([10, 8])],\n",
       " [107.68285191260543, 61.19526261748122, dict_values([10, 10])]]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "#create initial DecisionTree model\n",
    "dt = DecisionTreeRegressor()\n",
    "#create ParaGrid for cross validation\n",
    "paramGrid_dt = ParamGridBuilder() \\\n",
    "    .addGrid(dt.maxDepth, [1, 2, 5, 10]) \\\n",
    "    .addGrid(dt.maxBins, [4, 5, 8, 10]) \\\n",
    "    .build()\n",
    "pipeline_dt = Pipeline(stages = [sqlTrans_1, assembler_1, dt])\n",
    "\n",
    "#create cross validator for two different metrics\n",
    "dt_cv_1 = CrossValidator(estimator = pipeline_dt,\n",
    "                          estimatorParamMaps = paramGrid_dt,\n",
    "                          evaluator = RegressionEvaluator(metricName='rmse'),\n",
    "                          numFolds=5)\n",
    "dt_cv_2 = CrossValidator(estimator = pipeline_dt,\n",
    "                          estimatorParamMaps = paramGrid_dt,\n",
    "                          evaluator = RegressionEvaluator(metricName='mae'),\n",
    "                          numFolds=5)\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "dt_cv_model1 = dt_cv_1.fit(train)\n",
    "dt_cv_model2 = dt_cv_2.fit(train)\n",
    "\n",
    "#print cv results for model selection\n",
    "dt_list = []\n",
    "for i in range(len(paramGrid_dt)):\n",
    "    dt_list.append([dt_cv_model1.avgMetrics[i], dt_cv_model2.avgMetrics[i], paramGrid_dt[i].values()])\n",
    "dt_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd3b152-bc7d-4403-a4a5-3bd787634a9b",
   "metadata": {},
   "source": [
    "Metric RMSE and MAE showed same results in choose best parameters. So either models could be considered as best model for linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8ae24c98-fcb3-4f5a-be47-6427f8196a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_best = dt_cv_model1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6039dcf4-d24d-46b6-ab6a-ff0760dc5bab",
   "metadata": {},
   "source": [
    "## 3.5. Gradient-Boosted Trees (GBTs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5df805d9-adf3-4e4b-bf8b-20e1e335a468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[156.36913725524218, 111.90174726256305, dict_values([1, 4])],\n",
       " [162.45438598897243, 113.0311278129147, dict_values([1, 5])],\n",
       " [147.0625434284022, 105.92187880257805, dict_values([1, 8])],\n",
       " [151.41591979283132, 104.12063177489956, dict_values([1, 10])],\n",
       " [117.05876055865777, 79.1402497312001, dict_values([2, 4])],\n",
       " [117.31475219718337, 78.41860869144597, dict_values([2, 5])],\n",
       " [110.16540161729506, 77.06563233695758, dict_values([2, 8])],\n",
       " [102.21056917559324, 70.69748510595282, dict_values([2, 10])],\n",
       " [104.3634759529403, 68.66846119146763, dict_values([5, 4])],\n",
       " [110.72172185687107, 71.45111925931445, dict_values([5, 5])],\n",
       " [103.39034729727173, 61.460600348751655, dict_values([5, 8])],\n",
       " [92.18896441649525, 55.540543923198435, dict_values([5, 10])],\n",
       " [102.58416821775408, 67.58016496021523, dict_values([10, 4])],\n",
       " [109.66238057416153, 70.99627729900511, dict_values([10, 5])],\n",
       " [102.86516975990864, 60.545103362200145, dict_values([10, 8])],\n",
       " [107.68193496247541, 61.184996070802114, dict_values([10, 10])]]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "#create initial GBTs model\n",
    "gbt = GBTRegressor()\n",
    "#create ParaGrid for cross validation\n",
    "paramGrid_gbt = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxDepth, [1, 2, 5, 10]) \\\n",
    "    .addGrid(gbt.maxBins, [4, 5, 8, 10]) \\\n",
    "    .build()\n",
    "pipeline_gbt = Pipeline(stages = [sqlTrans_1, assembler_1, gbt])\n",
    "\n",
    "#create cross validator for two different metrics\n",
    "gbt_cv_1 = CrossValidator(estimator = pipeline_gbt,\n",
    "                          estimatorParamMaps = paramGrid_gbt,\n",
    "                          evaluator = RegressionEvaluator(metricName='rmse'),\n",
    "                          numFolds=5)\n",
    "gbt_cv_2 = CrossValidator(estimator = pipeline_gbt,\n",
    "                          estimatorParamMaps = paramGrid_gbt,\n",
    "                          evaluator = RegressionEvaluator(metricName='mae'),\n",
    "                          numFolds=5)\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "gbt_cv_model1 = gbt_cv_1.fit(train)\n",
    "gbt_cv_model2 = gbt_cv_2.fit(train)\n",
    "\n",
    "#print cv results for model selection\n",
    "gbt_list = []\n",
    "for i in range(len(paramGrid_gbt)):\n",
    "    gbt_list.append([gbt_cv_model1.avgMetrics[i], gbt_cv_model2.avgMetrics[i], paramGrid_gbt[i].values()])\n",
    "gbt_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9d8508-a7af-4288-a1d6-620b6f604596",
   "metadata": {},
   "source": [
    "Metric RMSE and MAE showed same results in choose best parameters. So either models could be considered as best model for linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "35e1cda4-36c8-4262-9863-c75230a34b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_best = gbt_cv_model1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d08c0d-6f7f-4cfd-b616-0d01a9b78a13",
   "metadata": {},
   "source": [
    "# 4. Model testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd27875-a1a1-4d87-8d83-e1bd15dcbb0e",
   "metadata": {},
   "source": [
    "In the last, I will evaluate the best models from each class on the test set. Compare the test_error of each models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "66422d10-876f-4ea9-be99-3cda1fa012b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+------------------+------------------+------------------+-----+--------------------+------------------+\n",
      "|Length1|Length2|Length3| Height|         log_width|        interacted|       poly_height|label|            features|        prediction|\n",
      "+-------+-------+-------+-------+------------------+------------------+------------------+-----+--------------------+------------------+\n",
      "|   26.3|   29.0|   33.5|  12.73|1.4941392880706375|             762.7|162.05290000000002|363.0|[26.3,29.0,33.5,1...|334.28851347878185|\n",
      "|   29.1|   31.5|   36.4|13.7592| 1.474305238442604| 916.6500000000001|      189.31558464|500.0|[29.1,31.5,36.4,1...|448.08479850158983|\n",
      "|   29.4|   32.0|   37.2|14.9544|1.6430274154276556|             940.8|      223.63407936|600.0|[29.4,32.0,37.2,1...| 568.8604897057808|\n",
      "|   29.4|   32.0|   37.2| 15.438|1.7191887763932197|             940.8|238.33184400000002|600.0|[29.4,32.0,37.2,1...| 634.4453093327111|\n",
      "|   30.4|   33.0|   38.3|14.8604| 1.664948302361668|1003.1999999999999|220.83148816000002|700.0|[30.4,33.0,38.3,1...| 585.3630642472054|\n",
      "+-------+-------+-------+-------+------------------+------------------+------------------+-----+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# linear regression model\n",
    "lr_best.transform(test).show(5)\n",
    "lr_test_error = RegressionEvaluator().evaluate(lr_best.transform(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "21762930-7c3d-41f8-a921-a41c77e20380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+------------------+------------------+------------------+-----+--------------------+-----------------+\n",
      "|Length1|Length2|Length3| Height|         log_width|        interacted|       poly_height|label|            features|       prediction|\n",
      "+-------+-------+-------+-------+------------------+------------------+------------------+-----+--------------------+-----------------+\n",
      "|   26.3|   29.0|   33.5|  12.73|1.4941392880706375|             762.7|162.05290000000002|363.0|[26.3,29.0,33.5,1...|330.2763140532298|\n",
      "|   29.1|   31.5|   36.4|13.7592| 1.474305238442604| 916.6500000000001|      189.31558464|500.0|[29.1,31.5,36.4,1...|447.5664645354469|\n",
      "|   29.4|   32.0|   37.2|14.9544|1.6430274154276556|             940.8|      223.63407936|600.0|[29.4,32.0,37.2,1...|569.2546668753753|\n",
      "|   29.4|   32.0|   37.2| 15.438|1.7191887763932197|             940.8|238.33184400000002|600.0|[29.4,32.0,37.2,1...|635.9476129169128|\n",
      "|   30.4|   33.0|   38.3|14.8604| 1.664948302361668|1003.1999999999999|220.83148816000002|700.0|[30.4,33.0,38.3,1...|586.2645599585537|\n",
      "+-------+-------+-------+-------+------------------+------------------+------------------+-----+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lasso regression model\n",
    "la_best.transform(test).show(5)\n",
    "la_test_error = RegressionEvaluator().evaluate(la_best.transform(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e44ebc73-6244-453c-a7ba-9fae8eddfc04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+------------------+------------------+------------------+-----+--------------------+------------------+\n",
      "|Length1|Length2|Length3| Height|         log_width|        interacted|       poly_height|label|            features|        prediction|\n",
      "+-------+-------+-------+-------+------------------+------------------+------------------+-----+--------------------+------------------+\n",
      "|   26.3|   29.0|   33.5|  12.73|1.4941392880706375|             762.7|162.05290000000002|363.0|[26.3,29.0,33.5,1...|334.28851347878185|\n",
      "|   29.1|   31.5|   36.4|13.7592| 1.474305238442604| 916.6500000000001|      189.31558464|500.0|[29.1,31.5,36.4,1...|448.08479850158983|\n",
      "|   29.4|   32.0|   37.2|14.9544|1.6430274154276556|             940.8|      223.63407936|600.0|[29.4,32.0,37.2,1...| 568.8604897057808|\n",
      "|   29.4|   32.0|   37.2| 15.438|1.7191887763932197|             940.8|238.33184400000002|600.0|[29.4,32.0,37.2,1...| 634.4453093327111|\n",
      "|   30.4|   33.0|   38.3|14.8604| 1.664948302361668|1003.1999999999999|220.83148816000002|700.0|[30.4,33.0,38.3,1...| 585.3630642472054|\n",
      "+-------+-------+-------+-------+------------------+------------------+------------------+-----+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ElasticNet Regression\n",
    "en_best.transform(test).show(5)\n",
    "en_test_error = RegressionEvaluator().evaluate(en_best.transform(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b5ea3ed6-842a-4e22-91b6-fa00fa13e291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+------------------+-----+--------------------+----------+\n",
      "|Length1|Length2|Length3| Height|         log_width|label|            features|prediction|\n",
      "+-------+-------+-------+-------+------------------+-----+--------------------+----------+\n",
      "|   26.3|   29.0|   33.5|  12.73|1.4941392880706375|363.0|[26.3,29.0,33.5,1...|     390.0|\n",
      "|   29.1|   31.5|   36.4|13.7592| 1.474305238442604|500.0|[29.1,31.5,36.4,1...|     390.0|\n",
      "|   29.4|   32.0|   37.2|14.9544|1.6430274154276556|600.0|[29.4,32.0,37.2,1...|     485.9|\n",
      "|   29.4|   32.0|   37.2| 15.438|1.7191887763932197|600.0|[29.4,32.0,37.2,1...|     610.0|\n",
      "|   30.4|   33.0|   38.3|14.8604| 1.664948302361668|700.0|[30.4,33.0,38.3,1...|     485.9|\n",
      "+-------+-------+-------+-------+------------------+-----+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decision tree\n",
    "dt_best.transform(test).show(5)\n",
    "dt_test_error = RegressionEvaluator().evaluate(dt_best.transform(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cc6ebbcd-6a5b-4c14-9522-ae3ec031ee3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+------------------+-----+--------------------+------------------+\n",
      "|Length1|Length2|Length3| Height|         log_width|label|            features|        prediction|\n",
      "+-------+-------+-------+-------+------------------+-----+--------------------+------------------+\n",
      "|   26.3|   29.0|   33.5|  12.73|1.4941392880706375|363.0|[26.3,29.0,33.5,1...|392.82534762144195|\n",
      "|   29.1|   31.5|   36.4|13.7592| 1.474305238442604|500.0|[29.1,31.5,36.4,1...| 404.8448709134235|\n",
      "|   29.4|   32.0|   37.2|14.9544|1.6430274154276556|600.0|[29.4,32.0,37.2,1...| 499.6066411595096|\n",
      "|   29.4|   32.0|   37.2| 15.438|1.7191887763932197|600.0|[29.4,32.0,37.2,1...| 613.7742114756664|\n",
      "|   30.4|   33.0|   38.3|14.8604| 1.664948302361668|700.0|[30.4,33.0,38.3,1...|496.51787724051997|\n",
      "+-------+-------+-------+-------+------------------+-----+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gradient-Boosted Trees (GBTs)\n",
    "gbt_best.transform(test).show(5)\n",
    "gbt_test_error = RegressionEvaluator().evaluate(gbt_best.transform(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "400ca45f-e586-4a1d-a227-a7786ff5295a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.72791190392779 70.32578975740446 69.72791190392779 100.52487007272099 97.91809918197725\n"
     ]
    }
   ],
   "source": [
    "print(lr_test_error, la_test_error, en_test_error, dt_test_error, gbt_test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96257dc-7a4d-4b29-9498-b8df1dea7c1f",
   "metadata": {},
   "source": [
    "We could see that linear regression and elasticNet regression models showing lowest test error, could be considered as best models over the other models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
